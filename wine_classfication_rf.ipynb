{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including sklearn, pandas, and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Wine Dataset\n",
    "Load the wine dataset from sklearn datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine dataset\n",
    "wine = datasets.load_wine()\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "wine_df = pd.DataFrame(data=np.c_[wine['data'], wine['target']],\n",
    "                       columns=wine['feature_names'] + ['target'])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "Explore the dataset to understand its structure and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the dataset\n",
    "# Display the shape of the DataFrame\n",
    "print(f\"Dataset shape: {wine_df.shape}\")\n",
    "\n",
    "# Display the column names\n",
    "print(f\"Columns: {wine_df.columns}\")\n",
    "\n",
    "# Display the data types of each column\n",
    "print(f\"Data types:\\n{wine_df.dtypes}\")\n",
    "\n",
    "# Display the summary statistics of the DataFrame\n",
    "print(f\"Summary statistics:\\n{wine_df.describe()}\")\n",
    "\n",
    "# Display the count of each target value\n",
    "print(f\"Target counts:\\n{wine_df['target'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Preprocess the data by handling missing values and scaling features if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(f\"Missing values in each column:\\n{wine_df.isnull().sum()}\")\n",
    "\n",
    "# Since there are no missing values, we don't need to handle them\n",
    "\n",
    "# Check if scaling is necessary\n",
    "# We can do this by looking at the range of each feature\n",
    "print(f\"Range of each feature:\\n{wine_df[wine.feature_names].max() - wine_df[wine.feature_names].min()}\")\n",
    "\n",
    "# If the range is very large for some features, we might need to scale them\n",
    "# However, in this case, all features are within a reasonable range, so we don't need to scale them\n",
    "\n",
    "# Split the data into features and target\n",
    "X = wine_df[wine.feature_names]\n",
    "y = wine_df['target']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and test sets\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split\n",
    "Split the dataset into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = wine_df[wine.feature_names]\n",
    "y = wine_df['target']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and test sets\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Train the Random Forest Model\n",
    "Create a Random Forest classifier and train it on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Evaluate the performance of the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model on the test data\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate the balanced accuracy score\n",
    "balanced_accuracy = metrics.balanced_accuracy_score(y_test, y_pred)\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy}\")\n",
    "\n",
    "# Calculate the precision, recall, f1-score\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
